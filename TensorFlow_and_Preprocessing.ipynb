{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E3IuJosWMG7",
        "outputId": "7c5b0eb3-1929-432f-83da-e5b9423dd779"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import datasets, layers, models, metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn utilities\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# sklearn models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6026040f",
        "outputId": "acdf43bb-c9aa-462e-e915-4c2bdd34cb74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory not found: data/Brain_Cancer raw MRI data/Brain_Cancer/brain_tumor\n",
            "Image data shape: (4008, 256, 256, 3)\n",
            "Image labels shape: (4008,)\n",
            "Example labels: ['brain_glioma' 'brain_glioma' 'brain_glioma' 'brain_glioma'\n",
            " 'brain_glioma' 'brain_glioma' 'brain_glioma' 'brain_glioma'\n",
            " 'brain_glioma' 'brain_glioma']\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "#run this if images are not saved locally as np array\n",
        "\n",
        "#in order for this to work, you need to download the data from kaggle as a zip and upload it to the following directory:\n",
        "# /content/drive/MyDrive/CS3540/CS3540-Assignments/final-project/data/\n",
        "\n",
        "glioma_path = 'Brain_Cancer raw MRI data/Brain_Cancer/brain_glioma'\n",
        "menin_path = 'Brain_Cancer raw MRI data/Brain_Cancer/brain_menin'\n",
        "tumor_path = 'data/Brain_Cancer raw MRI data/Brain_Cancer/brain_tumor'\n",
        "\n",
        "image_data = []\n",
        "image_labels = []\n",
        "\n",
        "target_size = (256, 256) # this is a hyper param, rescaled size of images that will be used as the input shape for the CNN\n",
        "\n",
        "\n",
        "\n",
        "for directory_path in [glioma_path, menin_path, tumor_path]:\n",
        "    if os.path.exists(directory_path):\n",
        "        label = os.path.basename(directory_path) # directory corresponds to label\n",
        "        for filename in os.listdir(directory_path):\n",
        "            full_path = os.path.join(directory_path, filename)\n",
        "            if os.path.isfile(full_path):\n",
        "                try:\n",
        "                    img = Image.open(full_path).convert('RGB')\n",
        "                    img_resized = img.resize(target_size) # <- we resize so the image data isn't incredibly huge but still provides enough information to train on\n",
        "                    img_array = np.asarray(img_resized)\n",
        "                    image_data.append(img_array)\n",
        "                    image_labels.append(label)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {full_path}: {e}\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {directory_path}\")\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "image_data = np.array(image_data)\n",
        "image_labels = np.array(image_labels)\n",
        "\n",
        "print(\"Image data shape:\", image_data.shape)\n",
        "print(\"Image labels shape:\", image_labels.shape)\n",
        "print(\"Example labels:\", image_labels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1e809f5",
        "outputId": "9755067f-1e53-4e47-c398-68b5c8d711a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image data and labels saved as 'image_data.npy' and 'image_labels.npy'\n"
          ]
        }
      ],
      "source": [
        "# Save the image data and labels\n",
        "# only run this cell if generating images for the first time after running the above cell\n",
        "np.save('image_data.npy', image_data)\n",
        "np.save('image_labels.npy', image_labels)\n",
        "\n",
        "print(\"Image data and labels saved as 'image_data.npy' and 'image_labels.npy'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az7krrOsg69J"
      },
      "outputs": [],
      "source": [
        "# load image data, if saved locally\n",
        "\n",
        "image_data = np.load('image_data.npy')\n",
        "image_labels = np.load('image_labels.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "290b6bdd",
        "outputId": "c7b3285b-995a-4dbf-b991-bef0fb93f791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image data normalized.\n"
          ]
        }
      ],
      "source": [
        "# Normalize the image data\n",
        "\n",
        "# run this cell after images have either been loaded or generated in the above cell\n",
        "for image in image_data:\n",
        "  image = image / 255\n",
        "\n",
        "print(\"Image data normalized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eae76de4",
        "outputId": "d756d071-7dca-48d7-cd36-a83ac37fcb12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data split into training and testing sets.\n",
            "X_train shape: (4844, 256, 256, 3)\n",
            "X_test shape: (1212, 256, 256, 3)\n",
            "y_train shape: (4844,)\n",
            "y_test shape: (1212,)\n"
          ]
        }
      ],
      "source": [
        "#run this after image normalization\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_data, image_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data split into training and testing sets.\")\n",
        "print(\"X_train shape:\", x_train.shape)\n",
        "print(\"X_test shape:\", x_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW9JLjS69LJ4"
      },
      "outputs": [],
      "source": [
        "class_names = ['glioma', 'menin', 'tumor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIVkj-Xp92Qr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hkellner/Library/Mobile Documents/com~apple~CloudDocs/School/Machine Learning/Final_project/.venv/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# defining model\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3))) # input is of the form height, width, num colors\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(len(class_names)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw5IDGfW-nrZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#run this cell to change image labels to ints (CNN wants this and won't work without it)\n",
        "\n",
        "# change labels from strings to int\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the string labels to integer labels\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "\n",
        "# Update y_train and y_test to use the encoded labels\n",
        "y_train = y_train_encoded\n",
        "y_test = y_test_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "rU8nRRtVDPBR",
        "outputId": "f3510f0d-0d8f-4327-c2bb-2952546e4c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 711ms/step - accuracy: 0.3355 - loss: 1.0986 - val_accuracy: 0.3259 - val_loss: 1.0995\n",
            "Epoch 2/5\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 693ms/step - accuracy: 0.3412 - loss: 1.0985 - val_accuracy: 0.3259 - val_loss: 1.0995\n",
            "Epoch 3/5\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 680ms/step - accuracy: 0.3412 - loss: 1.0985 - val_accuracy: 0.3259 - val_loss: 1.0994\n",
            "Epoch 4/5\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 700ms/step - accuracy: 0.3412 - loss: 1.0985 - val_accuracy: 0.3259 - val_loss: 1.0993\n",
            "Epoch 5/5\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 711ms/step - accuracy: 0.3412 - loss: 1.0986 - val_accuracy: 0.3259 - val_loss: 1.0994\n"
          ]
        }
      ],
      "source": [
        "# training model\n",
        "\n",
        "# only run this if weights are not saved locally\n",
        "# this will take an extraordinarily long time (> 1 hour if you don't have colab pro)\n",
        "# if you do have colab pro, T4 gpus will speed this up quite a bit, should take ~1 minute.\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=5,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38/38 - 7s - 184ms/step - accuracy: 0.3259 - loss: 1.0993\n",
            "accuracy on test dataset:  0.3259075880050659\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# metrics + evaluation\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'testing accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "checkpoint_path = \"checkpoints.weights.h5\"\n",
        "model.save_weights(checkpoint_path)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(\"accuracy on test dataset: \", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suFk_05zheJh",
        "outputId": "5bbd1e6c-337d-4648-bbdb-5d0e8e20757a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hkellner/Library/Mobile Documents/com~apple~CloudDocs/School/Machine Learning/Final_project/.venv/lib/python3.13/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38/38 - 7s - 193ms/step - accuracy: 0.3259 - loss: 1.0993\n",
            "[1.0993390083312988, 0.3259075880050659]\n"
          ]
        }
      ],
      "source": [
        "#run this to load locally stored weights (won't work unless you have either trained once or have the weights saved to your drive)\n",
        "checkpoint_path = \"checkpoints.weights.h5\"\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(256, 256, 3))) # input is of the form height, width, num colors\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(len(class_names)))\n",
        "\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.evaluate(x_test,  y_test, verbose=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_vIUY_Giokh",
        "outputId": "733b8405-3679-4582-9d88-05c534f76164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 187ms/step\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Argument `segment_ids` and `data` should have same leading dimension. Got (3636,) v.s. (1212,).",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      9\u001b[39m f1 = tf.keras.metrics.F1Score(average=\u001b[33m'\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m AUC = tf.keras.metrics.AUC(\n\u001b[32m     11\u001b[39m     num_thresholds=\u001b[32m200\u001b[39m,\n\u001b[32m     12\u001b[39m     curve=\u001b[33m'\u001b[39m\u001b[33mROC\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     label_weights=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     20\u001b[39m     from_logits=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mAUC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m f1.update_state(y_true_one_hot, y_pred)\n\u001b[32m     24\u001b[39m false_positives.update_state(y_true_one_hot, y_pred)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/School/Machine Learning/Final_project/.venv/lib/python3.13/site-packages/keras/src/metrics/confusion_metrics.py:1357\u001b[39m, in \u001b[36mAUC.update_state\u001b[39m\u001b[34m(self, y_true, y_pred, sample_weight)\u001b[39m\n\u001b[32m   1354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._from_logits:\n\u001b[32m   1355\u001b[39m     y_pred = activations.sigmoid(y_pred)\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m \u001b[43mmetrics_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_confusion_matrix_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConfusionMatrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTRUE_POSITIVES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrue_positives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConfusionMatrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTRUE_NEGATIVES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrue_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConfusionMatrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFALSE_POSITIVES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfalse_positives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConfusionMatrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFALSE_NEGATIVES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfalse_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthresholds_distributed_evenly\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thresholds_distributed_evenly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/School/Machine Learning/Final_project/.venv/lib/python3.13/site-packages/keras/src/metrics/metrics_utils.py:484\u001b[39m, in \u001b[36mupdate_confusion_matrix_variables\u001b[39m\u001b[34m(variables_to_update, y_true, y_pred, thresholds, top_k, class_id, sample_weight, multi_label, label_weights, thresholds_distributed_evenly)\u001b[39m\n\u001b[32m    481\u001b[39m     y_pred = y_pred[..., class_id, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m thresholds_distributed_evenly:\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_update_confusion_matrix_variables_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariables_to_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthresholds_with_epsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthresholds_with_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m y_pred.shape:\n\u001b[32m    496\u001b[39m     pred_shape = ops.shape(y_pred)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/School/Machine Learning/Final_project/.venv/lib/python3.13/site-packages/keras/src/metrics/metrics_utils.py:275\u001b[39m, in \u001b[36m_update_confusion_matrix_variables_optimized\u001b[39m\u001b[34m(variables_to_update, y_true, y_pred, thresholds, multi_label, sample_weights, label_weights, thresholds_with_epsilon)\u001b[39m\n\u001b[32m    273\u001b[39m     fp = ops.transpose(ops.flip(ops.cumsum(ops.flip(fp_bucket_v), axis=\u001b[32m1\u001b[39m)))\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     tp_bucket_v = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43msegment_sum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbucket_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_segments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_thresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     fp_bucket_v = ops.segment_sum(\n\u001b[32m    281\u001b[39m         data=false_labels,\n\u001b[32m    282\u001b[39m         segment_ids=bucket_indices,\n\u001b[32m    283\u001b[39m         num_segments=num_thresholds,\n\u001b[32m    284\u001b[39m     )\n\u001b[32m    285\u001b[39m     tp = ops.flip(ops.cumsum(ops.flip(tp_bucket_v)))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/School/Machine Learning/Final_project/.venv/lib/python3.13/site-packages/keras/src/ops/math.py:83\u001b[39m, in \u001b[36msegment_sum\u001b[39m\u001b[34m(data, segment_ids, num_segments, sorted)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkeras.ops.segment_sum\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msegment_sum\u001b[39m(data, segment_ids, num_segments=\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28msorted\u001b[39m=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Computes the sum of segments in a tensor.\u001b[39;00m\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m \u001b[33;03m    array([3, 30, 300], dtype=int32)\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[43m_segment_reduce_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((data,)):\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m SegmentSum(num_segments, \u001b[38;5;28msorted\u001b[39m).symbolic_call(data, segment_ids)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/School/Machine Learning/Final_project/.venv/lib/python3.13/site-packages/keras/src/ops/math.py:27\u001b[39m, in \u001b[36m_segment_reduce_validation\u001b[39m\u001b[34m(data, segment_ids)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `segment_ids` should be an 1-D vector, got shape: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(segment_ids_shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Consider either flatten input with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mvectorize with vmap.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     23\u001b[39m     segment_ids_shape[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m data_shape[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m segment_ids_shape[\u001b[32m0\u001b[39m] != data_shape[\u001b[32m0\u001b[39m]\n\u001b[32m     26\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `segment_ids` and `data` should have same leading \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdimension. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment_ids_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m v.s. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: Argument `segment_ids` and `data` should have same leading dimension. Got (3636,) v.s. (1212,)."
          ]
        }
      ],
      "source": [
        "# evaluation: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score\n",
        "y_pred = model.predict(x_test)\n",
        "y_true = y_test\n",
        "\n",
        "#one hot to match y_true to y_pred (for multiple classes)\n",
        "y_true_one_hot = tf.one_hot(y_true, depth=len(class_names))\n",
        "false_positives = tf.keras.metrics.FalsePositives()\n",
        "false_negatives = tf.keras.metrics.FalseNegatives()\n",
        "f1 = tf.keras.metrics.F1Score(average='weighted')\n",
        "AUC = tf.keras.metrics.AUC(\n",
        "    num_thresholds=200,\n",
        "    curve='ROC',\n",
        "    summation_method='interpolation',\n",
        "    name=None,\n",
        "    dtype=None,\n",
        "    thresholds=None,\n",
        "    multi_label=False,\n",
        "    num_labels=None,\n",
        "    label_weights=None,\n",
        "    from_logits=False)\n",
        "\n",
        "AUC.update_state(y_test, y_pred)\n",
        "f1.update_state(y_true_one_hot, y_pred)\n",
        "false_positives.update_state(y_true_one_hot, y_pred)\n",
        "false_negatives.update_state(y_true_one_hot, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvW80k_SNd9R",
        "outputId": "693199b9-ba7a-4d5b-f98d-12c1ad0533a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 score:  0.16021591\n",
            "AUROC: AUC:  0.4944307\n",
            "false negatives:  1212.0\n",
            "false positives:  0.0\n"
          ]
        }
      ],
      "source": [
        "print(\"f1 score: \", f1.result().numpy())\n",
        "print(\"AUROC: AUC: \", AUC.result().numpy())\n",
        "print(\"false negatives: \", false_negatives.result().numpy())\n",
        "print(\"false positives: \", false_positives.result().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy0B6DBeQgfB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
